---
title: "Datasheet for the `COFFEE_COFFEE_COFFEE` dataset, version 0.1"
author: "Thomas Rosenthal, GitHub: [github.com/mrpotatocode](https://github.com/mrpotatocode), "
date: "March 15, 2021"
output: github_document
nocite: '@*'
bibliography: datasheets.bib
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation

### For what purpose was the dataset created?

This dataset was created to explore the relationship between the attributes of single origin coffees  (like where a coffee came from, what kind of coffee it is, how a coffee was processed, etc) and its tasting notes (how a coffee tastes after brewing). The dataset is intended to find trends in objective differences in coffee. High quality models derived from this dataset can be influential to both coffee producers/roasters (those who sell the coffee) and coffee farmers (those who grow the coffee). While similar datasets may exist, these appear to be closed-sourced or privately held (such as those that might be generated by an app, e.g. firstbloom). This dataset is gathered from coffee roaster websites, processed with open-source software, and stored in publicly available repositories.

### Who created the dataset and on behalf of which entity?

The dataset was created by Thomas Rosenthal (University of Toronto, Faculty of Information). This dataset is derived from the work of coffee shop website owners and designers. Thomas Rosenthal is independent of any organization and has built this dataset for his own personal use.

### Who funded the creation of the dataset?

This dataset is unfunded.

## Composition

### What do the instances that comprise the dataset represent?

Each instance is a single origin coffee and its corresponding metadata. Each coffee is a combination of qualitative values (such as the history of the farm from which it was grown) or quantitative values (such as the price it was sold at each stage of its journey from farm to cup). The number of attributes vary immensely across coffees and are generally disseminated by coffee roasters.

### How many instances are there in total?

The current dataset has roughly 350 coffees. More are added every week. The dataset needs to reach >1500 to be useful to machine learning models.

### Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?

A sample. It would not be possible to collect all possible coffees produced in a single year without incredible technical overhead (and even then, such a dataset is unlikely to be wholly representative). Coffees also change year to year, though some are reproduced on an annual basis. These are unlikely to taste exactly the same, even if grown at the same farm and processed in the same manner. 

Coffee selection occurs far upstream of this dataset. Coffees are bought and sold in lots based on their quality scores. This dataset does not usually capture quality score, as it is rarely provided to consumers. Many coffee lots are split between roasters, so it is possible to have the same coffee beans end up as two distinct coffees after roasting. As such, this is a sample of many available coffees, curated by a selection of roasters.

Roaster prioritization was slightly random; a combination of those that were well known, and whose data was easy to scrape week after week. Future data collection must continue to capture data for speciality (the top 20% of coffee produced) coffees, but determining precisely which roaster is not overly important until the dataset has matured.

### What data does each instance consist of?

An average coffee contains the following attributes: Roaster, Country of Origin, Region within that country, the coffee's Variety (botanical subspecies, similar to wine), the means by which it was Processed and its Tasting Notes. 

Other attributes can include: details about the farm (elevation, terroir, lat/long, etc), details about harvesting (picking methods, ripeness, etc), details about the farmer (personal histories, anecdotes), among others. 

It is required that at the very minimum, variety, country, and process must be known to predict tasting notes. Coffees that do not meet that standard are filtered out before modeling. Nearly all coffees in this dataset meet this minimum, though some do not. It is possible some coffees that do not meet these minimums might be filtered from the scraping process.

### Is there a label or target associated with each instance?

Yes, there are multiple labels associated with each instance. The labels are tasting notes. Tasting notes have been defined roughly by the [SCA](https://sca.coffee/research/coffee-tasters-flavor-wheel), but additional supplementary tasting notes have been defied manually. This supplemental tasting note mapping can be found [here](https://github.com/mrpotatocode/COFFEE_COFFEE_COFFEE/tree/main/inputs/data/Conformed). Both the SCA and supplemental mapping treat tasting notes in a hierarchy of Trait>Group>Note (for example, "Strawberry" is Fruity>Berries>Strawberry). Code for joining the tasting notes and the dataset requires stemming. Models can target any of these levels, though more data is required to target notes. 

### Is any information missing from individual instances?

In its current form, highly qualitative data (such as stories) is captured in HTML files, but not brought into CSV files (which are used to create this dataset). Additionally, some attributes are not presented consistently across roasters or across weeks. Webscraping assumes a degree of stagnant form, so high variance can cause attributes to be missed. Many coffees are presented with the same standard set of attributes, but almost no coffee is presented with all possible attributes (e.g. some roasters may list a completely unique attribute). Depending on sparseness, some attributes are not used in models, but the data is still generally collected. 

### Are relationships between individual instances made explicit?

Generally, columns are kept consistent even if exact language is not consistent across data sources (e.g. "Processing" is the same as "Process"). Relationships across CSV files and between each individual coffee should be explicitly understood once a common set of coffee terms is learned. It would benefit dataset users to read a book such as [The World Atlas of Coffee](https://www.theworldatlasofcoffee.com/). The level of detail provided within this book is mirrored in the dataset.

### Are there recommended data splits (e.g., training, development/validation, testing)?

Not currently. In the future, a stratified split would be useful in order to capture all countries within both training and testing sets. For example, if the dataset is sorted by the date of collection (made aware by file name labels), certain countries would be likely be missed in train models as they may not have produced coffees in the first two thirds of the year. Similarly, should sufficient data exist to use region as a model criterion, region would need to be present in both train and test sets. This may seem obvious; however some regions may only produce coffees once a year in countries that produce coffees twice a year. This difference is not wholly apparent by the dataset, though would be derivable. 

### Are there any errors, sources of noise, or redundancies in the dataset?

Yes. This dataset is highly variable in its instances. Webscraping is, by nature, a poor means to collect data that is maintained by humans. Certain formats are not maintained consistently on websites and input errors are extremely probable. Sometimes columns may be mapped to the wrong data, especially as the values scraped are occasionally placed in different order. The webscrapers have been designed to handle frequent cases. 

Some tasting notes were _not_ derivable without regex, and the success of this varied. As such, some rows are extremely noisy and would benefit from manual cleaning. Manual cleaning is difficultâ€”the webscrapers do not seek to scrape unique coffees, but rather all coffees present at a given time. This means that a coffee present on a website for two weeks will be scraped twice (if scraping is weekly) and stored distinctly. This redundancy can be removed by excluding file name (which indicates when it was scraped: the coffee is otherwise identical). However, should manual cleaning occur in one of the two CSV files, these coffees will no longer be identical.

Additionally, webscrapers capture data as it is presented, not as it actually is. It has been observed that data listed on coffee bags is different than data listed on websites. These errors cannot be addressed by technical means. 

Every effort has been made to predict and minimize errors, but this is simply not possible in the long run. If errors are discovered in the dataset, changes should be made with a thoughtful means to avoid duplication within the dataset.

### Is the dataset self-contained, or does it link to or otherwise rely on external resources?

The dataset is self-contained, but its growth relies on the webscrapers that produce additional CSV files for future coffees. GitHub Actions is currently used as a means to execute scripts on a regular basis, but other services could be built if this GitHub Actions was no longer an option. It is assumed that the numerous R packages required to run these scrapers will remain usable (especially by GitHub Actions, which connects to these packages via CRAN). Nothing would inhibit the use of already scraped data. 

### Does the dataset contain data that might be considered confidential?

No. All data is available from other sources. All coffee-theory is likely available from your nearest SCA-trained barista. Go say hi. 

### Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?

No, though excessive caffeine consumption is known to cause anxiety. Please drink responsibly.

### Does the dataset relate to people?

Not directly. The dataset may refer to the farmers, producers, and/or roasters who were involved in its production. This is by name only. 

### Does the dataset identify any subpopulations (e.g., by age, gender)?

No. Even with the name of these individuals, no demographic data is kept with the dataset (though it may be possible to derive these things with more sophisticated methods, e.g. using computer vision to analyze photos of these individuals to determine their age/gender/etc). This would be beyond the intention of this project.

### Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?

Yes. It is possible to know the names of farmers and producers who contributed to the coffee, but only if provided by the roasters. These names are sometimes accompanied by a personal story and/or anecdote about their life as it relates to coffee. It is presumed that these stories are intended to be read, and that these stories were given to roasters with informed consent. It might be possible to learn of specific locations of these farms within a region.

### Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?

No. It is possible that major shifts in government in coffee growing regions could potentially affect farmers or producers within this dataset. It is extremely unlikely that this dataset or its underlying raw HTML files would be sufficiently detailed to aid in their identification.

## Collection Process

###  How was the data associated with each instance acquired?

Currently, the dataset is comprised of five, R-based webscrapers (with several more in development). These webscrapers visit a variety of coffee roaster and wholesale distributor websites and gather the attributes of interest for each coffee. The webscrapers are given a parent URL (usually a collection or the current offerings), and each corresponding coffee URL is scraped. The parent website is saved as an HTML file. For example, if a Roaster has four coffees available: Ethiopia A, Kenya B, Costa Rica C, Brazil D, the webscraper will produce a URL for each coffee: coffeewebsite/current_coffees/ethoipia-A, coffeewebsite/current_coffees/kenya-B, etc.

The proceeding child coffees are scraped for their available metadata. Some discrimination is made (like the exclusion of blends, espressos, decafs, etc) to filter out less relevant coffees. Coffee attributes are usually presented in the form of a table, but narrative form is not uncommon. Scrapers look for keywords and regex patterns to produce CSV files aligned with the values the scrapers have produced. Most coffee HTML files are also saved so scraping can be repeated if errors are discovered. Details of this process are [here](https://github.com/mrpotatocode/COFFEE_COFFEE_COFFEE/blob/main/journal/Week3/Week3.md).

Determining which coffee roasters to use has been based on personal preference. In the long run, as many roasters as possible is preferred. SCA "specialty" criteria should be used to determine if a roaster belongs within the dataset (that is, within the top 20% of all coffees produced annually). Highly transparent roasters should be given preference for future webscrapers (e.g. those who provide the best data and have philosophically aligned themselves with transparency in sourcing and pricing).

### What mechanisms or procedures were used to collect the data?

Data is/was scraped from various coffee websites. The scraping is scheduled and performed automatically by GitHub Actions. On a weekly basis, GitHub Actions runs each scraper separately (one scraper per Roaster website). These scripts, along with their output CSV and HTML files are saved within a different repo: [`Automatic_Drip`](https://github.com/mrpotatocode/Automatic_Drip). The scraping process saves the HTML file of each coffee available at a given time within that website. These HTML files are then processed, cleaned, and saved as CSV files alongside the original HTML files. The dataset is built from this series of CSV files. The CSV files can be combined, de-duplicated, and analyzed any anytime in any language. 

To combine the CSV files, R and python codes are provided here:
```
#R code:
#required packages
library(tidyverse)
library(data.table)

#function for fread (data.table)
read_plus <- function(flnm) {
    read_csv(flnm) %>% 
        mutate(filename = flnm) %>% 
        rename_with(str_to_title)}

#specify your local path (path = "...")
raw_data <- 
  list.files(path = "../..",
    pattern = "*.csv",
    full.names = T) %>% 
  map_df(~read_plus(.))

raw_data.head()  
```

```
#python code:
#required packages
import pandas as pd
import os

#change dir to your local path
os.chdir("../..")

#looping mechanism
#specify your local path (os.walk("..."))
file_list = []
for dirname, _, filenames in os.walk("/"):
    for filename in filenames:
        if '.csv' in filename:
            file_list.append(os.path.join(dirname, filename))

raw_data = (pd.concat(pd.read_csv(f, encoding='latin-1') for f in file_list))

raw_data.head
```

### If the dataset is a sample from a larger set, what was the sampling strategy?

Sampling was not done with any stratification. All coffees available for sale on any given week are scraped. The scrapers were first produced based on personal preference and second on ease of designing a robust scraper. Future iterations should aim to produce as much good data, with the least amount of maintenance for scrapers.

### Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?

Data scraping and data processing scripts were written by Thomas Rosenthal. His gracious partner manually compiled a small dataset to provide a bigger base of data to model from. She was not compensated for this task. The process was overseen by Rohan Alexander as an incredible academic advisor. He was not compensated directly for this task. 

It is not known how the data producers (i.e. the website maintainers from which the coffee was scraped) were compensated. Websites were not compensated for data scraping. Should the dataset be used in a commercial setting rather than a personal one, websites should be contacted and compensated for this data.

### Over what timeframe was the data collected?

The data collection process started in January of 2021. It is ongoing and not expected to stop until at least 2022. The dataset will benefit from at least one full year of coffee production, as some countries do not produce coffees year-round. Single origin coffees generally take 9-12 months to be sold for consumption. As such, data collected in 2021 is representative of coffees grown in 2020.

### Were any ethical review processes conducted?

No formal processes were conducted. This project is built on the labour of many who do not benefit directly from its outcomes. Ethical implications have been written about and will continue to be considered at all stages of the process.

### Did you collect the data from individuals directly, or obtain it via third parties or other sources (e.g., websites)?

All data was collected from websites. Future iterations would like to work more directly with coffee roasters, in a continuous relationship rather than from anonymous scraping mechanisms. Data provided by coffee roasters directly might include slightly different attributes but would also be more reliable on a week-by-week basis.

### Were the individuals in question notified about the data collection?

No. Website owners may have mechanisms to see that their website is being scraped. One web-designer requested that raw HTML files from their website not be stored in public repos. This request was obliged. 

### Did the individuals in question consent to the collection and use of their data?

The data is not about the behaviours of any individuals. Consent of collection of data was not obtained, but this data is also widely available (from multiple sources of the same coffee). 

Consent was not properly obtained for farmer/producer names being stored within a dataset. Relationships with roasters directly would help address this gap.

### If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?

Should any coffee roaster wish to be excluded from this analysis, these requests would be honoured, and the data would be deleted. Should any individual farmer or producer whose name appears in this dataset no longer wish to be included, these requests would be honoured, and the data would be deleted. 

### Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?

No. This does not seem to be applicable as of now. Should the dataset be used in a commercial context, it might be worthwhile to consider what repercussions utilization of this data might have on existing relationships with farmers and/or producers.

## Preprocessing/cleaning/labeling

### Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?

Yes. Some coffees have been filtered out (for example, blends, decafs, and espressos). These coffees are not scraped, and thus cannot be added back into the dataset later (coffees are almost never stored on roaster websites after they are no longer for sale). Missing values are occasionally derivable from other sources (for example, if country is not stated outright, but region can lead to its creation, this will likely be implemented in future builds). In some scrapers, coffee URLs were built from the name of coffee as it was presented, but this did not always resolve in that coffee's URL (almost always due to a typo made on the website). These coffees are accidentally filtered out. Later scraper builds addressed this.

Some stemming is required to group tasting notes (e.g. treating 'strawberry' and 'strawberries' as the same note). This is not done during scraping. Furthermore, in order to keep the supplemental tasting notes table up to date (see composition), exclusionary (anti) joins should compare the dataset to the tasting notes table as tasting notes are highly variable and hard to predict. Updates to the tasting notes table are manually done (i.e. a tasting note presented in the coffee dataset is added to the tasting note table for future mapping). It is expected that this will become less laborious over time.

### Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?

Yes. Nearly all source data from scraped websites are saved as HTML files. Some websites contained Google Maps API keys and were thus not saved within the public repo. It is intended to remove these API keys in future scrapers automatically and be able to save the HTML files.

### Is the software used to preprocess/clean/label the instances available?

All scraping codes were built in R and is available in the [Automatic_Drip](https://github.com/mrpotatocode/Automatic_Drip) repository.

Additional model code has not yet been made available but is expected to reside within the `COFFEE_COFFEE_COFFEE` repository. Many cleaning processes have been built in order to develop these models, so anyone using the dataset for modelling purposes will likely want to incorporate those steps as well.

## Uses

### Has the dataset been used for any tasks already?

The dataset has been used in a small draft binomial model as of March. One shiny app has been built from this dataset, which is publicly available [here](https://mrpotatocode.shinyapps.io/FavouriteCoffees/). Additional shiny apps will likely be built. These are all for personal use and the current [shinyapps.io](https://www.shinyapps.io/#pricing) subscription does not support permanent use of these shiny apps.

### Is there a repository that links to any or all papers or systems that use the dataset?

No papers have been published based on this dataset (nor is there any intention to). 

### What (other) tasks could the dataset be used for?

This dataset could be used to influence single origin coffee if linked with sales or personal preference data. Single origin coffees serve an affluent consumer base, and while the industry is currently dominated by independent coffee roasters, larger institutions ([Starbucks](https://www.starbucksreserve.com/en-us), [Nestle](https://www.nespresso.com/us/en/order/capsules/original/original-single-origin-assortment),  [Peet's](https://www.peets.com/collections/single-origin)) have become increasingly interested in single origin coffees. These institutions may seek to use consumer metrics alongside models similar to those producible from this dataset. However, it is unlikely they would use this dataset. 

Farmers, producers, and the independent coffee roasters who have direct relationships may also be interested in this dataset and its models. Outcomes may be useful explore how certain combinations might limit the volatility involved in producing high quality and distinctly tasting coffees. Coffee roasters may also already have a dataset of similar nature based on their own coffees. 

In its current state and scope, other meaningful tasks are difficult to predict.

### Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?

Yes. The emphasis has been on tasting notes. However, coffee data, especially its rich qualitative attributes have not been processed. Other uses might want to incorporate this. It is not believed that there are any stark assumptions that might have otherwise influenced this dataset's creation. 

### Are there tasks for which the dataset should not be used?

None currently. If a problematic use is learned of, this section will be updated.

## Distribution

### Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?

Yes, the dataset is publicly available.

### How will the dataset be distributed?

It is distributed via the current repository.

### When will the dataset be distributed?

It is presently available.

### Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?

The dataset and code have been released under an [MIT License](https://opensource.org/licenses/MIT). It is preferred that this dataset _not_ be used for commercial purposes without consent from its creator, but this is not legally binding nor required. The purpose of this dataset is to celebrate and explore coffee, rather than seek greater profitability.

### Have any third parties imposed IP-based or other restrictions on the data associated with the instances?

No. Scraping of websites is potentially against some websites terms of service, which may result in IP-based restrictions if done without consideration. Scrapers have a built-in delay and are scheduled to run at different times. Most coffee websites do not have explicit terms of service, but this may change at any point. 

The services building the dataset (GitHub Actions) may also change their terms of service. Current documentation for GitHub Actions is located [here](https://docs.github.com/en/actions).

### Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?

None currently or expected in the future.

## Maintenance

### Who is supporting/hosting/maintaining the dataset?

The creators of the dataset are supporting and maintaining it. The dataset is hosted in a personal repository held by Thomas Rosenthal.

### How can the owner/curator/manager of the dataset be contacted (e.g., email address)?

Thomas Rosenthal can be contacted via [email](t.rose.github@protonmail.com).

###  Is there an erratum?

Not currently or expected in the future. Please create issues through GitHub in order to have them resolved.

### Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?

New instances will continue to be added on a weekly basis (or faster, where time permits). Corrections will be made to future instances, but not likely applied to previously collected instances. The source code can be altered to reproduce CSV files from HTML files with some small changes, if one was inclined to do so. The tasting note mapping table is updated on GitHub every time additions are made to it.

Models built from this dataset are intended to be rerun with ease, so new developments will generally work to enrich the robustness of the dataset, rather than add new features.

### If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)?

This dataset only indirectly relates to people. Farmer and producer names are scraped when provided, however, these names are not connected with any personal details (aside from what those farmers have provided already alongside their coffees). 

These farmers/producers may have pre-existing agreements with roasters about how long they expect their names to are present on their websites. If this is the case, efforts will be made to obfuscate names after a reasonable period of time.

### Will older versions of the dataset continue to be supported/hosted/maintained?

This dataset is designed to grow over time. Its underlying CSV sources are hosted within the [`Automatic_Drip`](https://github.com/mrpotatocode/Automatic_Drip) repository and R code to combine them into a single dataset is maintained and distributed within the [`COFFEE_COFFEE_COFFEE`](https://github.com/mrpotatocode/COFFEE_COFFEE_COFFEE) repository. A sample loading mechanism is provided within this datasheet. 

The dataset will be maintained as long as there is active interest in the project and no major terms of services affect the data collection. It is possible to work with any combinations of underlying CSV files and most scraped HTML files are saved alongside these CSV files for added transparency and reproducibility.

### If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?

Yes, additions to this dataset would be welcome, provided they followed formatting and respected the intention of this dataset as exploratory, rather than commercial. Contact via [email](t.rose.github@protonmail.com) is preferred. Additions would be incorporated via GitHub.

# References